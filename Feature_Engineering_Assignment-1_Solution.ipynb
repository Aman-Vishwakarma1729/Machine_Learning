{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270e8497-4230-4534-9180-28c9ea5904c0",
   "metadata": {},
   "source": [
    "#### Answer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdaaa9-08f0-402c-9eff-4761ed988e6e",
   "metadata": {},
   "source": [
    "Missing values in a dataset are values that are not present in the data for certain observations or variables. These values are typically denoted by \"NA,\" \"NaN,\" or simply left blank in the dataset. Missing values can occur for a variety of reasons, such as data entry errors, data processing issues, or intentional missingness.\n",
    "\n",
    "It is essential to handle missing values because they can impact the accuracy and validity of statistical analyses and machine learning models. Missing values can lead to biased estimates and reduced predictive power, and can even cause errors in the calculations.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "* decision trees \n",
    "* random forests \n",
    "* support vector machines\n",
    "\n",
    "These algorithms are capable of handling missing values without imputation or other data cleaning techniques. Other algorithms, such as k-nearest neighbors and linear regression, may require imputation or other techniques to handle missing values effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c7332-e5af-47ad-a13b-e56550b7a935",
   "metadata": {},
   "source": [
    "#### Answer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b90c0-9ee7-40b7-a1c0-a285d1bb24b2",
   "metadata": {},
   "source": [
    "* Deletion: Deleting missing values from the dataset. This technique is applicable when the number of missing values is small or when there is no systematic bias in the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1550b2d-f16b-467f-a79c-0461503c1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8d255-568a-4dcc-b9a2-3440725df883",
   "metadata": {},
   "source": [
    "* Imputation: Filling in missing values with estimated values. This technique is applicable when the missing data is systematic and has a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455f7c8-79dd-4d4c-bf3a-ee1044694c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.fillna(df.mean(), inplace=True) ## filling missing data with mean.\n",
    "                                   ## We can also fill it using median or mode depends on certain conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b70b2-0a30-4481-b4f7-412f953e9453",
   "metadata": {},
   "source": [
    "* Prediction: Using a model to predict the missing values based on the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bad09d-a499-439c-ab17-e6ffb5411726",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using K-Nearest Neighbourhood\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89a105-85d7-4761-a901-82ee3af7e59c",
   "metadata": {},
   "source": [
    "* Interpolation: Filling in missing values by interpolation based on the neighboring values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ba6fa-c5c0-4702-a285-05e4a4cdfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3c39c-599a-463e-ba55-d32ca4051d4d",
   "metadata": {},
   "source": [
    "* Multiple imputation: Generating multiple imputed datasets and combining them to obtain a final dataset. This technique is applicable when there is a significant amount of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf9459-50bc-4f0d-9c3c-fa72501fa98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faab66a-e086-4723-a371-2b3a22868666",
   "metadata": {},
   "source": [
    "#### Answer_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1f4a1-a228-4ce7-a065-d5dcd0b812e8",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of classes in a dataset is not equal, meaning that one class is underrepresented compared to the others. This is a common issue in many real-world datasets, where one class may be much rarer than the others. For example, in a medical dataset, the number of patients who have a rare disease may be much smaller than the number of patients who do not have the disease.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to biased and inaccurate models. When a machine learning algorithm is trained on imbalanced data, it may have a tendency to favor the majority class, leading to poor performance on the minority class. This can result in false negatives (i.e., incorrectly identifying a member of the minority class as a member of the majority class) and false positives (i.e., incorrectly identifying a member of the majority class as a member of the minority class).\n",
    "\n",
    "For example, consider a fraud detection system that is trained on a dataset with 99% non-fraudulent transactions and only 1% fraudulent transactions. If the algorithm is not designed to handle imbalanced data, it may simply classify all transactions as non-fraudulent, leading to a high rate of false negatives (i.e., fraudulent transactions that are not detected) and a low rate of true positives (i.e., correctly detected fraudulent transactions).\n",
    "\n",
    "To address imbalanced data, techniques such as undersampling (i.e., randomly removing some samples from the majority class), oversampling (i.e., creating synthetic samples of the minority class), and using specialized algorithms that are designed to handle imbalanced data can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a6461-88bb-4fbe-9f26-128f624d2e6e",
   "metadata": {},
   "source": [
    "#### Answer_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12545f7c-a799-430f-b284-6c95b47c196c",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two common techniques used to address imbalanced data in machine learning.\n",
    "\n",
    "* Down-sampling involves reducing the number of samples in the majority class to balance the class distribution with the minority class. This can be done by randomly selecting a subset of the majority class samples to match the number of samples in the minority class. For example, in a binary classification problem with 80% negative samples and 20% positive samples, down-sampling would involve randomly selecting a subset of 20% of the negative samples to match the number of positive samples.\n",
    "\n",
    "* Up-sampling involves increasing the number of samples in the minority class to balance the class distribution with the majority class. This can be done by creating synthetic samples of the minority class using techniques like SMOTE (Synthetic Minority Over-sampling Technique), which involves interpolating between existing minority class samples to create new synthetic samples. For example, in a binary classification problem with 80% negative samples and 20% positive samples, up-sampling would involve creating synthetic samples of the positive class to match the number of negative samples.\n",
    "\n",
    "Whether to use up-sampling or down-sampling depends on the specific problem and the dataset. Down-sampling may be appropriate when the majority class has a much larger number of samples than the minority class, and the goal is to balance the dataset by removing some of the majority class samples. Up-sampling may be appropriate when the minority class has a small number of samples and the goal is to generate more samples to balance the dataset.\n",
    "\n",
    "For example, in a credit card fraud detection problem, the number of fraudulent transactions may be much smaller than the number of non-fraudulent transactions. In this case, up-sampling techniques like SMOTE can be used to generate synthetic samples of the minority class to balance the dataset. Conversely, in a customer churn prediction problem, there may be a large number of non-churn customers and a relatively small number of churn customers. In this case, down-sampling techniques can be used to randomly select a subset of the non-churn customers to balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58597ce-7fe5-4736-bf3c-7273a6c54bcf",
   "metadata": {},
   "source": [
    "#### Answer_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbd120-1e35-431f-bb82-4b665bc476cf",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used in machine learning to artificially increase the size of a dataset by creating new training examples from the existing data. The goal of data augmentation is to introduce additional variations in the training data, which can help improve the performance and robustness of machine learning models.\n",
    "\n",
    "One common technique used for data augmentation is SMOTE (Synthetic Minority Over-sampling Technique), which is specifically designed to address the problem of imbalanced data. SMOTE works by creating synthetic examples of the minority class by interpolating between existing examples.\n",
    "\n",
    "To explain how SMOTE works, let's consider a binary classification problem with two classes: positive and negative. Suppose that the dataset is imbalanced, with only a few positive examples and many more negative examples. SMOTE works by first selecting a positive example from the dataset. It then selects one of its k nearest neighbors (i.e., one of the k examples in the dataset that are closest to the selected positive example) and creates a new synthetic example by interpolating between the selected positive example and its nearest neighbor. The synthetic example is created by randomly selecting a point along the line segment connecting the two examples. This process is repeated until the desired number of synthetic examples has been created.\n",
    "\n",
    "By using SMOTE to create synthetic examples of the minority class, we can increase the number of positive examples in the dataset and balance the class distribution. This can help improve the performance of machine learning models trained on imbalanced datasets, especially in cases where the minority class is difficult to identify due to its low representation in the dataset. However, it's important to note that SMOTE should be used with caution, as it can also introduce noise and overfitting if not applied appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19143492-3313-4b41-b43f-a132a497c8b7",
   "metadata": {},
   "source": [
    "#### Answer_6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d29c3-1f6b-4e48-93da-6f3f711fdec9",
   "metadata": {},
   "source": [
    "Outliers are data points that are significantly different from other data points in a dataset. These are extreme values that lie far from the majority of the data and can have a significant impact on the analysis and modeling of the data.\n",
    "\n",
    "Outliers can occur due to various reasons such as measurement errors, experimental errors, data entry errors, or simply natural variation in the data. They can have a significant impact on statistical measures such as the mean, standard deviation, and correlation, leading to biased estimates and incorrect conclusions.\n",
    "\n",
    "It is essential to handle outliers in a dataset for several reasons:\n",
    "\n",
    "* Outliers can distort the statistical measures: Outliers can significantly affect the mean and standard deviation, leading to biased estimates of the central tendency and variability of the data. This can affect the accuracy of statistical analyses and modeling.\n",
    "\n",
    "* Outliers can affect the distribution of data: Outliers can cause the data to deviate from a normal distribution, making it difficult to apply certain statistical tests and assumptions.\n",
    "\n",
    "* Outliers can affect the performance of machine learning models: Outliers can have a significant impact on the performance of machine learning models, especially those that are sensitive to the distribution of data or rely on distance metrics.\n",
    "\n",
    "* Outliers can be indicative of underlying issues: Outliers can sometimes indicate errors in the data or underlying issues in the process that generated the data. Identifying and addressing these issues can help improve the accuracy and reliability of the data.\n",
    "\n",
    "Handling outliers involves various techniques such as removing them from the dataset, replacing them with more representative values, or transforming the data to reduce the impact of outliers. The choice of technique depends on the specific problem and the nature of the outliers. However, it is important to handle outliers carefully, as their removal or modification can significantly affect the analysis and modeling of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3184b-4107-453b-9674-36eeb0c6c020",
   "metadata": {},
   "source": [
    "#### Answer_7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9e283-8873-4ad6-af1d-83dc67a6af35",
   "metadata": {},
   "source": [
    "* Deleting missing data: In some cases, it may be appropriate to simply remove the rows or columns with missing data from the dataset. However, this should be done carefully, as it can lead to biased results and loss of information if the missing data is not random.\n",
    "\n",
    "* Imputing missing data: This involves replacing the missing values with estimated values based on the available data. There are various imputation methods, such as mean imputation, mode imputation, regression imputation, and K-nearest neighbor imputation, among others.\n",
    "\n",
    "* Using statistical models: In some cases, statistical models can be used to predict the missing values based on the available data. For example, regression models can be used to predict missing values based on other variables in the dataset.\n",
    "\n",
    "* Multiple imputation: This involves generating multiple imputed datasets and analyzing them separately, then combining the results to obtain an overall estimate. Multiple imputation is a more robust approach than single imputation, as it takes into account the uncertainty associated with the missing values.\n",
    "\n",
    "* Domain-specific knowledge: In some cases, domain-specific knowledge can be used to fill in the missing data. For example, if the missing data is related to customer income, information about the customer's occupation, education, or location can be used to estimate their income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fff3a0-24e2-4466-a5e8-d8a00efe9106",
   "metadata": {},
   "source": [
    "#### Answer_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec48ee-3f01-447e-8a32-c80456e38ae1",
   "metadata": {},
   "source": [
    "* Visual inspection: One way to identify the pattern of missing data is to visually inspect the dataset. This can be done by creating scatter plots, histograms, or other visualizations of the data. If the missing data appears to be randomly distributed across the dataset, it may be MAR. However, if the missing data appears to be clustered or related to other variables, it may be MNAR.\n",
    "\n",
    "* Statistical tests: Various statistical tests can be used to determine the pattern of missing data. For example, Little's MCAR test can be used to test the hypothesis that the missing data is MCAR. If the test fails to reject the null hypothesis, it suggests that the missing data is MAR.\n",
    "\n",
    "* Correlation analysis: Correlation analysis can be used to determine if the missing data is related to other variables in the dataset. If the missing data is related to other variables, it may be MNAR.\n",
    "\n",
    "* Imputation: Imputation methods can also be used to determine the pattern of missing data. For example, if mean imputation results in biased estimates, it suggests that the missing data is MNAR.\n",
    "\n",
    "* Expert knowledge: Expert knowledge can also be used to determine the pattern of missing data. For example, if the missing data is related to a specific time period or demographic group, it may be MNAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66c300-74c9-47e5-a42b-928d687fde5f",
   "metadata": {},
   "source": [
    "#### Answer_9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdea7d1-264b-410b-8d80-4689cc66531a",
   "metadata": {},
   "source": [
    "* Use evaluation metrics that are appropriate for imbalanced datasets: Standard metrics such as accuracy, precision, and recall may not be appropriate for imbalanced datasets. Instead, evaluation metrics such as F1-score, area under the receiver operating characteristic curve (AUC-ROC), and area under the precision-recall curve (AUC-PR) are more suitable for evaluating the performance of machine learning models on imbalanced datasets.\n",
    "\n",
    "* Resampling: Resampling techniques such as oversampling the minority class or undersampling the majority class can be used to balance the dataset. This can help to improve the model's performance on the minority class, but it can also result in overfitting if not done carefully.\n",
    "\n",
    "* Adjust the classification threshold: By default, the classification threshold is set at 0.5, but this can be adjusted to balance the trade-off between the true positive rate and the false positive rate. For imbalanced datasets, it may be more appropriate to set a higher threshold to increase the specificity, which can help reduce the number of false positives.\n",
    "\n",
    "* Ensemble learning: Ensemble learning techniques such as bagging and boosting can be used to improve the model's performance on the minority class by combining multiple models.\n",
    "\n",
    "* Use domain-specific knowledge: In some cases, domain-specific knowledge can be used to improve the model's performance on the minority class. For example, if certain features are known to be more important for predicting the minority class, they can be given more weight in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d36ff1-0ce7-4c82-86be-c8f4212013ea",
   "metadata": {},
   "source": [
    "#### Answer_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e41fcd-8d9f-4eda-8fe3-109b212e2b5b",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset with the majority class, down-sampling the majority class is a common approach. Here are some methods that can be used to down-sample the majority class:\n",
    "\n",
    "* Random under-sampling: This involves randomly selecting a subset of the majority class samples to match the size of the minority class. This method is simple to implement, but it may lead to information loss and result in a biased model.\n",
    "\n",
    "* Cluster-based under-sampling: This involves identifying clusters of majority class samples and selecting representative samples from each cluster to match the size of the minority class. This method can be more effective than random under-sampling but may be more computationally expensive.\n",
    "\n",
    "* Tomek links: Tomek links are pairs of samples from different classes that are close to each other but are classified incorrectly by the classifier. Removing the majority class samples in Tomek links can help improve the performance of the classifier.\n",
    "\n",
    "* NearMiss algorithm: This algorithm selects the majority class samples that are closest to the minority class samples and removes them. There are different versions of the algorithm, such as NearMiss-1 and NearMiss-2, that differ in their selection criteria.\n",
    "\n",
    "It is important to note that down-sampling can result in information loss and may not be appropriate in all cases. In some cases, up-sampling the minority class or using other methods such as synthetic minority oversampling technique (SMOTE) may be more appropriate. It is also recommended to use appropriate evaluation metrics, such as F1-score, precision-recall curve, and AUC-ROC, to evaluate the performance of the model on the balanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850b980-51b0-4bc4-838e-948f00799e38",
   "metadata": {},
   "source": [
    "#### Answer_11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b183b6-3a1d-45c8-b999-9ce972f1072c",
   "metadata": {},
   "source": [
    "When dealing with a dataset that has a low percentage of occurrences, the minority class is typically up-sampled to balance the dataset. Here are some methods that can be used to up-sample the minority class:\n",
    "\n",
    "* Random over-sampling: This involves randomly duplicating minority class samples to match the size of the majority class. This method is simple to implement, but it may lead to overfitting and poor generalization performance.\n",
    "\n",
    "* SMOTE (Synthetic Minority Over-sampling Technique): This is a popular method that involves generating synthetic samples based on the minority class samples. SMOTE works by selecting a minority class sample, identifying its k nearest neighbors, and creating synthetic samples by interpolating between the selected sample and its neighbors. SMOTE can help to improve the model's performance on the minority class, but it may also result in the generation of unrealistic samples.\n",
    "\n",
    "* ADASYN (Adaptive Synthetic Sampling): This is an extension of the SMOTE algorithm that adjusts the degree of synthetic sample generation based on the difficulty of learning the minority class. ADASYN generates more synthetic samples in regions where the classification boundary is ambiguous and fewer synthetic samples in regions where the boundary is well-defined.\n",
    "\n",
    "* SMOTE-ENN (SMOTE and Edited Nearest Neighbors): This is a combination of over-sampling with SMOTE and under-sampling with Edited Nearest Neighbors (ENN). SMOTE is used to generate synthetic samples for the minority class, and ENN is used to remove the noisy samples from both the majority and minority classes.\n",
    "\n",
    "It is important to note that up-sampling the minority class can result in overfitting and may not be appropriate in all cases. In some cases, down-sampling the majority class or using other methods such as cost-sensitive learning may be more appropriate. It is also recommended to use appropriate evaluation metrics, such as F1-score, precision-recall curve, and AUC-ROC, to evaluate the performance of the model on the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11159e-77ac-455f-808a-1be1b6119a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab95a6-2d8b-4b97-808f-29cab72bacc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da7829-c75c-4a59-b381-6a047d648323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7951b-16c2-4d0d-95f1-b5d03536887f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47334fc-6087-4a11-a46b-862d691cd6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
