{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b9e70b-1cd6-41f3-9776-f1798f0ded0e",
   "metadata": {},
   "source": [
    "#### Answer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf6d0d-07f4-4d8b-b9a4-bc972b11e38c",
   "metadata": {},
   "source": [
    "* Min-Max scaling, also known as normalization, is a technique used in data preprocessing to scale numerical features to a fixed range. The scaled range is typically between 0 and 1 but can be adjusted to any desired range. This technique is particularly useful when the dataset has values that are not normally distributed, and their magnitudes are significantly different. Scaling the values between 0 and 1 helps to ensure that all features are on a similar scale, making it easier for models to interpret and compare them.\n",
    "* The Min-Max scaling formula is as follows:\n",
    "\n",
    "* > X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "* Where X is the original feature, X_min is the minimum value in the feature, and X_max is the maximum value in the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d740963d-56f2-4a1c-b9cb-529817a41ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9572cd0d-bc7d-42f6-bf04-5ba3c4a6e1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "028c8879-47fb-486f-ade2-fd2c70e081bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['normalized_sepal_length'] = (iris['sepal_length']-iris['sepal_length'].min())/(iris['sepal_length'].max()-iris['sepal_length'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5eb15b1-5c8a-4c8a-be29-977edab1c580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.222222\n",
       "1      0.166667\n",
       "2      0.111111\n",
       "3      0.083333\n",
       "4      0.194444\n",
       "         ...   \n",
       "145    0.666667\n",
       "146    0.555556\n",
       "147    0.611111\n",
       "148    0.527778\n",
       "149    0.444444\n",
       "Name: normalized_sepal_length, Length: 150, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['normalized_sepal_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75b619-cbfd-4acd-b08a-f47cfbef85eb",
   "metadata": {},
   "source": [
    "#### Answer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0175b-1805-43d4-91a8-1f6b87fb494b",
   "metadata": {},
   "source": [
    "* The Unit Vector technique, also known as normalization, is another technique used in feature scaling to rescale numerical features to have a magnitude of 1. This technique scales the values in each feature based on their Euclidean distance, resulting in a unit vector for each feature. Unlike Min-Max scaling, which scales the values to a fixed range, the Unit Vector technique scales the values to have a unit magnitude, making it particularly useful when the magnitude of the features is important but their absolute values are not. This technique is commonly used in machine learning models that rely on distance calculations, such as K-Nearest Neighbors.\n",
    "\n",
    "* The Unit Vector scaling formula is as follows:\n",
    "\n",
    "* > X_normalized = X / ||X||\n",
    "\n",
    "* Where X is the original feature, and ||X|| is the Euclidean distance of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81023426-4b4c-49f5-8bdd-345827cd29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f03c2d-f04b-406d-a144-18986adeeb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0832602e-be81-41ae-9933-207fc874becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3735ff3-f9dd-4622-9635-c03618203f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82451335, 0.5658425 ],\n",
       "       [0.8528513 , 0.52215386],\n",
       "       [0.82659925, 0.56279098],\n",
       "       [0.82926643, 0.55885346],\n",
       "       [0.81153434, 0.58430473],\n",
       "       [0.81067923, 0.58549055],\n",
       "       [0.80417614, 0.59439106],\n",
       "       [0.8269265 , 0.56231002],\n",
       "       [0.8349582 , 0.55031336],\n",
       "       [0.84507884, 0.53464171],\n",
       "       [0.82493237, 0.56523144],\n",
       "       [0.81602448, 0.57801734],\n",
       "       [0.8479983 , 0.52999894],\n",
       "       [0.82012695, 0.5721816 ],\n",
       "       [0.82321279, 0.56773296],\n",
       "       [0.79159032, 0.61105218],\n",
       "       [0.81067923, 0.58549055],\n",
       "       [0.82451335, 0.5658425 ],\n",
       "       [0.83205029, 0.5547002 ],\n",
       "       [0.80188283, 0.59748132],\n",
       "       [0.84623284, 0.53281327],\n",
       "       [0.80942185, 0.58722762],\n",
       "       [0.787505  , 0.61630826],\n",
       "       [0.83957016, 0.54325128],\n",
       "       [0.81602448, 0.57801734],\n",
       "       [0.85749293, 0.51449576],\n",
       "       [0.8269265 , 0.56231002],\n",
       "       [0.82958775, 0.55837637],\n",
       "       [0.83696961, 0.54724936],\n",
       "       [0.82659925, 0.56279098],\n",
       "       [0.84003938, 0.54252543],\n",
       "       [0.84623284, 0.53281327],\n",
       "       [0.78526917, 0.61915453],\n",
       "       [0.79476781, 0.6069136 ],\n",
       "       [0.84507884, 0.53464171],\n",
       "       [0.8422714 , 0.5390537 ],\n",
       "       [0.84366149, 0.53687549],\n",
       "       [0.80588181, 0.59207643],\n",
       "       [0.82622734, 0.56333682],\n",
       "       [0.83205029, 0.5547002 ],\n",
       "       [0.81923192, 0.57346234],\n",
       "       [0.89043468, 0.45511106],\n",
       "       [0.80873608, 0.5881717 ],\n",
       "       [0.81923192, 0.57346234],\n",
       "       [0.80188283, 0.59748132],\n",
       "       [0.8479983 , 0.52999894],\n",
       "       [0.80188283, 0.59748132],\n",
       "       [0.8209052 , 0.57106449],\n",
       "       [0.81995808, 0.57242357],\n",
       "       [0.83460941, 0.55084221],\n",
       "       [0.90947448, 0.41575976],\n",
       "       [0.89442719, 0.4472136 ],\n",
       "       [0.9121687 , 0.40981492],\n",
       "       [0.92257988, 0.38580613],\n",
       "       [0.91841262, 0.3956239 ],\n",
       "       [0.89755433, 0.44090388],\n",
       "       [0.88583154, 0.46400699],\n",
       "       [0.89806271, 0.43986745],\n",
       "       [0.91551945, 0.4022737 ],\n",
       "       [0.88749608, 0.46081527],\n",
       "       [0.92847669, 0.37139068],\n",
       "       [0.89138513, 0.45324668],\n",
       "       [0.93887632, 0.34425465],\n",
       "       [0.903134  , 0.42935879],\n",
       "       [0.88799441, 0.45985425],\n",
       "       [0.90756252, 0.41991699],\n",
       "       [0.88147997, 0.47222141],\n",
       "       [0.90658206, 0.42202958],\n",
       "       [0.94242775, 0.33440985],\n",
       "       [0.91313788, 0.40765084],\n",
       "       [0.87903186, 0.47676304],\n",
       "       [0.90882955, 0.41716766],\n",
       "       [0.92949071, 0.36884552],\n",
       "       [0.90882955, 0.41716766],\n",
       "       [0.91085325, 0.41273038],\n",
       "       [0.91036648, 0.41380294],\n",
       "       [0.9246781 , 0.38074981],\n",
       "       [0.91268458, 0.40866474],\n",
       "       [0.90034895, 0.43516866],\n",
       "       [0.90981905, 0.41500518],\n",
       "       [0.91653938, 0.39994446],\n",
       "       [0.91653938, 0.39994446],\n",
       "       [0.90658206, 0.42202958],\n",
       "       [0.91192151, 0.41036468],\n",
       "       [0.87415728, 0.48564293],\n",
       "       [0.87002219, 0.49301257],\n",
       "       [0.90756252, 0.41991699],\n",
       "       [0.93935732, 0.34293997],\n",
       "       [0.88147997, 0.47222141],\n",
       "       [0.91036648, 0.41380294],\n",
       "       [0.90407227, 0.42737962],\n",
       "       [0.89734997, 0.44131966],\n",
       "       [0.91250932, 0.4090559 ],\n",
       "       [0.90849045, 0.41790561],\n",
       "       [0.90076868, 0.43429919],\n",
       "       [0.88491822, 0.46574643],\n",
       "       [0.89127803, 0.45345724],\n",
       "       [0.90580954, 0.42368511],\n",
       "       [0.89792072, 0.44015722],\n",
       "       [0.89755433, 0.44090388],\n",
       "       [0.88583154, 0.46400699],\n",
       "       [0.90658206, 0.42202958],\n",
       "       [0.92114622, 0.38921671],\n",
       "       [0.90838094, 0.41814361],\n",
       "       [0.90795938, 0.41905818],\n",
       "       [0.93015522, 0.36716653],\n",
       "       [0.89076187, 0.45447034],\n",
       "       [0.92935209, 0.36919466],\n",
       "       [0.93690259, 0.34959052],\n",
       "       [0.89442719, 0.4472136 ],\n",
       "       [0.89717068, 0.44168403],\n",
       "       [0.92136416, 0.38870051],\n",
       "       [0.9149178 , 0.40364021],\n",
       "       [0.91578821, 0.4016615 ],\n",
       "       [0.90055164, 0.43474907],\n",
       "       [0.89442719, 0.4472136 ],\n",
       "       [0.90795938, 0.41905818],\n",
       "       [0.89674427, 0.44254912],\n",
       "       [0.94744567, 0.31991672],\n",
       "       [0.93887632, 0.34425465],\n",
       "       [0.90718823, 0.42072498],\n",
       "       [0.89442719, 0.4472136 ],\n",
       "       [0.93979342, 0.34174306],\n",
       "       [0.91914503, 0.3939193 ],\n",
       "       [0.89708903, 0.44184982],\n",
       "       [0.91381155, 0.40613847],\n",
       "       [0.9113706 , 0.41158672],\n",
       "       [0.89734997, 0.44131966],\n",
       "       [0.91615733, 0.40081883],\n",
       "       [0.92307692, 0.38461538],\n",
       "       [0.93528626, 0.3538921 ],\n",
       "       [0.90116674, 0.43347261],\n",
       "       [0.91615733, 0.40081883],\n",
       "       [0.91381155, 0.40613847],\n",
       "       [0.91992326, 0.39209844],\n",
       "       [0.93177739, 0.36303015],\n",
       "       [0.88002217, 0.4749326 ],\n",
       "       [0.89998132, 0.43592845],\n",
       "       [0.89442719, 0.4472136 ],\n",
       "       [0.9121687 , 0.40981492],\n",
       "       [0.90756252, 0.41991699],\n",
       "       [0.9121687 , 0.40981492],\n",
       "       [0.90658206, 0.42202958],\n",
       "       [0.9048187 , 0.42579704],\n",
       "       [0.89708903, 0.44184982],\n",
       "       [0.91268458, 0.40866474],\n",
       "       [0.92949071, 0.36884552],\n",
       "       [0.90795938, 0.41905818],\n",
       "       [0.87681241, 0.48083261],\n",
       "       [0.89138513, 0.45324668]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(iris[['sepal_length','sepal_width']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4483d0-ddae-4477-9653-418abe915009",
   "metadata": {},
   "source": [
    "#### Answer_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b4a64-19cb-4f1c-971b-fc79c1fc00b1",
   "metadata": {},
   "source": [
    "Principle Component Analysis (PCA) is a technique used in data analysis and dimensionality reduction to transform a dataset into a lower-dimensional space while retaining most of the original variation in the data. PCA works by identifying the directions in the data that explain the most variance and projecting the data onto those directions to form a new set of uncorrelated variables, known as principal components.\n",
    "\n",
    "PCA is commonly used in dimensionality reduction to simplify complex datasets with many variables while preserving most of the information in the original dataset. The principal components can be used to visualize and analyze the data, build predictive models, or perform other data analysis tasks.\n",
    "\n",
    "The steps for performing PCA are as follows:\n",
    "\n",
    "* Standardize the data by subtracting the mean and dividing by the standard deviation.\n",
    "* Compute the covariance matrix of the standardized data.\n",
    "* Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "* Select the top k eigenvectors that explain the most variance.\n",
    "* Project the data onto the selected eigenvectors to obtain a new set of principal components.\n",
    "\n",
    "* For example, suppose you have a dataset with five variables:\n",
    "\n",
    "X1, X2, X3, X4, X5\n",
    "\n",
    "Using PCA, we can reduce the dimensionality of this dataset by projecting it onto a lower-dimensional space while retaining most of the variation in the data. Suppose we compute the covariance matrix of the standardized data and obtain the following eigenvalues and eigenvectors:\n",
    "\n",
    "Eigenvalues:\n",
    "2.5, 1.5, 1.0, 0.5, 0.0\n",
    "\n",
    "Eigenvectors:\n",
    "0.5, 0.5, 0.4, 0.4, 0.4\n",
    "0.4, 0.4, -0.4, -0.4, 0.6\n",
    "0.3, 0.3, 0.6, -0.6, -0.3\n",
    "0.4, -0.4, 0.1, -0.1, 0.8\n",
    "0.6, -0.6, -0.5, 0.5, 0.0\n",
    "\n",
    "The eigenvalues represent the amount of variance explained by each eigenvector, and the eigenvectors represent the directions in the data that explain the most variance. We can select the top k eigenvectors, say the first three, and project the data onto those eigenvectors to obtain a new set of principal components:\n",
    "\n",
    "PC1 = 0.5*X1 + 0.4*X2 + 0.3*X3 + 0.4*X4 + 0.6*X5\n",
    "PC2 = 0.5*X1 + 0.4*X2 + 0.3*X3 - 0.4*X4 - 0.6*X5\n",
    "PC3 = 0.4*X1 - 0.4*X2 + 0.6*X3 + 0.1*X4 - 0.5*X5\n",
    "\n",
    "The resulting principal components can be used to visualize and analyze the data, build predictive models, or perform other data analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e977c18-27dd-4eba-b27f-f78fc008bb6b",
   "metadata": {},
   "source": [
    "#### Answer_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d316f6f-05d6-4a4b-9d16-0824451a5a5f",
   "metadata": {},
   "source": [
    "PCA can be used for feature extraction, which involves selecting a subset of the original features in a dataset that are most relevant to the task at hand. Feature extraction is often used in machine learning to reduce the dimensionality of the input data, remove noise and redundancy, and improve the performance of predictive models.\n",
    "\n",
    "PCA is a powerful technique for feature extraction because it can identify the most important features in a dataset by finding the directions in the data that explain the most variance. By projecting the data onto those directions, PCA creates a new set of uncorrelated variables, called principal components, that can be used as features for further analysis or modeling.\n",
    "\n",
    "For example, suppose we have a dataset with 100 variables, and we want to identify the most important features for predicting a target variable. We can use PCA to extract the top k principal components that explain the most variance in the data, and use those components as features for a predictive model.\n",
    "\n",
    "Here's a step-by-step example of how PCA can be used for feature extraction:\n",
    "\n",
    "* Standardize the data by subtracting the mean and dividing by the standard deviation.\n",
    "* Compute the covariance matrix of the standardized data.\n",
    "* Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "* Select the top k eigenvectors that explain the most variance.\n",
    "* Project the data onto the selected eigenvectors to obtain a new set of principal components.\n",
    "* Use the principal components as features for a predictive model.\n",
    "\n",
    "For instance, let's assume we have a dataset of images with 100 pixels each. Each pixel is a feature, and we want to identify the most important features for classifying the images into two categories: \"cat\" or \"dog\". We can use PCA to extract the top 20 principal components that explain the most variation in the images, and use those components as features for a classification model.\n",
    "\n",
    "After applying PCA, we get a new set of 20 principal components, each of which is a linear combination of the original pixel features. We can then use those principal components as input features for a classification model, such as logistic regression, support vector machine, or neural network. By using the principal components as features, we can reduce the dimensionality of the input data, remove noise and redundancy, and improve the performance of the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9132651-1e81-4bb1-a86e-3e2de9f01ce0",
   "metadata": {},
   "source": [
    "#### Answer_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb702da-3aee-4d2e-86a6-d025891f69b7",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data by scaling the features so that they are all on the same scale, with values between 0 and 1.\n",
    "\n",
    "Here's how you could use Min-Max scaling to preprocess the features in the dataset:\n",
    "\n",
    "* Identify the features that need to be scaled, such as price, rating, and delivery time.\n",
    "\n",
    "* For each feature, calculate the minimum and maximum values in the dataset.\n",
    "\n",
    "* Use the following formula to scale the values of each feature between 0 and 1:\n",
    "\n",
    "* scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "This formula maps the minimum value of the feature to 0 and the maximum value to 1, while preserving the relative distances between the values.\n",
    "\n",
    "Replace the original values of each feature with their scaled values.\n",
    "\n",
    "After Min-Max scaling, the features in the dataset will all be on the same scale, with values between 0 and 1. This is important for building a recommendation system because it ensures that no one feature dominates the others and that each feature contributes equally to the overall similarity or dissimilarity between items.\n",
    "\n",
    "For example, suppose we have a dataset of food delivery orders, where the price of the order ranges from rs.10 to rs.50, the rating ranges from 1 to 5 stars, and the delivery time ranges from 20 to 60 minutes. We can use Min-Max scaling to preprocess these features so that they are all on the same scale. After scaling, the price feature will have values between 0 and 1, the rating feature will have values between 0 and 1, and the delivery time feature will have values between 0 and 1. This will ensure that each feature contributes equally to the overall similarity or dissimilarity between food delivery orders, and that no one feature dominates the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1d124-dbed-46d5-a2db-a7c94c9cd16d",
   "metadata": {},
   "source": [
    "#### Answer_6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320c1f0-3d7c-4d7e-b94a-d32aceb85641",
   "metadata": {},
   "source": [
    "PCA can be used to reduce the dimensionality of a dataset with many features, such as financial data and market trends, by identifying the most important features that explain the most variance in the data. Here's how you could use PCA to reduce the dimensionality of the dataset for predicting stock prices:\n",
    "\n",
    "* Standardize the data by subtracting the mean and dividing by the standard deviation. This step ensures that all features are on the same scale, which is important for PCA to work properly.\n",
    "\n",
    "* Compute the covariance matrix of the standardized data. The covariance matrix describes the relationships between the features in the dataset, and is used to identify the directions of maximum variance in the data.\n",
    "\n",
    "* Compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance in the data, while the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "* Sort the eigenvectors by their corresponding eigenvalues, in descending order. The eigenvectors with the highest eigenvalues explain the most variance in the data, and therefore represent the most important features.\n",
    "\n",
    "* Select the top k eigenvectors that explain a sufficient amount of variance in the data. This involves setting a threshold for the total amount of variance explained by the selected eigenvectors. A common threshold is to select eigenvectors that explain at least 95% of the total variance in the data.\n",
    "\n",
    "* Project the data onto the selected eigenvectors to obtain a new set of principal components. The principal components are uncorrelated variables that capture the most important information in the original features.\n",
    "\n",
    "* Use the principal components as input features for a predictive model. The number of input features is now reduced to the number of selected principal components, which is typically much smaller than the original number of features.\n",
    "\n",
    "For example, suppose we have a dataset of financial data and market trends for 100 different companies, each with 50 features. We can use PCA to identify the most important features that explain the most variance in the data, and reduce the dimensionality of the dataset to a smaller set of principal components. After applying PCA, we may find that the top 10 principal components capture 90% of the total variance in the data, which means that we can use those 10 principal components as input features for a predictive model to predict stock prices. This reduces the number of input features from 50 to 10, which can improve the performance of the predictive model and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545713d-2037-478b-8e9b-059ff60ffd70",
   "metadata": {},
   "source": [
    "#### Answer_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a3f9861-afd5-46c9-a1d1-29fb0ae0b18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [1, 5, 10, 15, 20]\n",
    "Min_Max_Normalize = []\n",
    "maxi = max(dataset)\n",
    "mini = min(dataset)\n",
    "\n",
    "for i in dataset:\n",
    "    norm = (i - mini)/(maxi - mini)\n",
    "    Min_Max_Normalize.append(norm)\n",
    "\n",
    "Min_Max_Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b46eb-daa7-4ae4-b7a6-c34c72658778",
   "metadata": {},
   "source": [
    "#### Answer_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361263e5-5b2f-4318-afe5-d6bc776f1a64",
   "metadata": {},
   "source": [
    "* The height, weight, and age features are likely to be important for predicting blood pressure, so we would want to retain these features in the dataset.\n",
    "* Gender may also be a useful feature, but it's not clear how it would be encoded (e.g., as a binary variable or as a categorical variable with more than two levels).\n",
    "* It's possible that some of the features are highly correlated with each other, in which case we would want to retain fewer principal components to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21394b7a-3373-4fd2-9a70-15353bd7eff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a1947-f8bc-49fb-8c5d-3b04d18ca772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386fa70-afd7-4c74-8d31-72254407ddcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58568be-4588-4afd-acb8-94fabe9103e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e402c9-d590-4c54-a89d-d362a67c88bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "775f5bca-a8c1-4f4e-a51f-928923d038d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
